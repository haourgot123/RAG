In this technical report, we present VinaLLaMA, an open-source, state-of-the-art (SOTA) Large Language Model for the Vietnamese language, built upon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only demonstrates fluency in Vietnamese but also exhibits a profound understanding of Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat, trained on 1-million high quality synthetic samples, achieves SOTA results on key benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking a significant advancement in the Vietnamese AI landscape and offering a versatile resource for various applications. 1 Introduction 
The surge in Large Language Models (LLMs) such as ChatGPT and GPT-4 has significantly advanced the field of artificial intelligence (AI), particularly in language processing. In 2023, Vietnam’s AI sector witnessed a notable development with the introduction of several Vietnamese-centric LLMs, including BLOOMZ’s Vietcuna, URA-LLaMA, PhoGPT, and dama-2. Amidst this progression, we introduce VinaLLaMA, a foundational LLM designed specifically for the Vietnamese language. VinaL- LaMA, built on top of LLaMA-2, represents a vital stride towards linguistic inclusivity in AI, adeptly addressing the syntactic and semantic intricacies of Vietnamese. Embracing the spirit of collaboration and open innovation, we are pleased to announce the open sourcing of VinaLLaMA-7B and its chat variant. These models are now accessible on HuggingFace, ensuring compatibility with all ’transformers’ framework-supported libraries. This endeavor not only contributes to the global AI research landscape but also provides a specialized tool for exploring and enhancing Vietnamese language processing, encouraging a wider engagement and application in AI- driven NLP research. 2 Related Work2. 1 Large Language Models. The growth of large language models (LLMs) start from Transformer [VSP+17], which was the base architecture to later pre-train BERT [CMCC23] and GPT [RNSS] with large-scale unsupervised data. These models lead to the appearance of popular foundation models RoBERTa [LOG+19], T5 [RSR+20] and BART [LLG+20]. Later, GPT-3 [BMR+20] perform the ability of few-shot and zero-shot learning through prompt engineering and in-context learning.Later, ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the two big milestone for not only in the field of language model, but also in aritifical intelligence and other fields. These models show the variety of skills with high quality output. These also lead to the raise of LLMs, which includes Llama [RSR+20], Llama-2 [TMS+23], Bloom [WSF+22], Falcon [AAA+23], Qwen [BBC+23] and Mistral [JSM+23]. 2. 2 Vietnamese Large Language Models. The landscape of Vietnamese Large Language Models has been evolving with significant contributions. Vietcuna [NPD23b] emerged as the pioneering model in this domain, initially undergoing further pre-training on the BLOOMZ [WSF+22] model. It was subsequently enhanced through fine-tuning with the OpenOrca-Viet [NPD23a] dataset, optimizing its performance in Vietnamese language tasks. In a different approach, URA-Llama [NTN+23] was developed, leveraging fine-tuning on a corpus of Vietnamese articles, building upon the foundational architecture of Llama-2. Furthermore, the PhoGPT [NNT+23] model represents another notable advancement. It was initially pre-trained on a substantial corpus of 41GB of text data, followed by a focused fine-tuning process utilizing a dataset of 150,000 samples, thereby broadening its capabilities in understanding and generating Vietnamese text. 2. 3 Alignment. To enhance the ability of language model, including in-context learning, reasoning, instruction learn- ing,. we need to align the model with some downstream dataset and supervised fine-tuning the LLMs on these dataset.Flan [WBZ+21] introduce instruction tuning and release the first public dataset which includes huge amount of different tasks with different templates. This improve the general performance of LLms a lot. Later, InstructGPT [OWJ+22] apply human feedback to modify the model output, help the model to generate the text with less toxic and higher quality, compare to other models. Vicuna and Guanaco include chat data from open-assistant dataset, which enhacne the chat ability of the model. Recently, since the ability of GPT-3,5 and GPT-4, many synthetic dataset that are generated from these models have been popularly applied such as: OpenOrca [WKM+22b], Dolphin [WKM+22a], OpenOrca-2 [WKM+22c], Platypus [LHR23]. 3 Training Procedure 3. 1 Pretraining 
LLaMA-2, a highly regarded pre-trained language model in English, shows a significant gap in handling Vietnamese-related content due to limited relevant tokens in its training set. Additionally, its original tokenizer falls short in multilingual applications. To address these issues, we compiled a new pretraining dataset combining public and synthetic in-house data. We selected BKAI’s LLaMA-2-chat tokenizer for the tokenizer, which has been specifically made for Vietnamese. This tokenizer shows enhanced performance in processing the Vietnamese language, making it a suitable choice for our VinaLLaMA model. 3. 1. 1 Public Data 
Books. The dataset encompassing Vietnamese literature in our study is comprehensive, comprising 250,000 volumes.This extensive collection spans various domains, including science, history, finance, and philosophy, as well as fiction genres like novels and science fiction, in addition to traditional Viet- namese literature. The distribution of these categories is methodically illustrated in Figure 1. Public News. The dataset under examination is derived from two principal Vietnamese news sources, VnExpress 1 and BaoMoi 2. This compilation encompasses an exhaustive collection of articles dis- seminated by these entities from January 1, 2010, through September 30, 2023. To align with ethical guidelines and content appropriateness standards, a filtration process was implemented, systematically excluding articles that contained keywords indicative of harmful or violent content. The inclusion of this public news data is crucial, as it helps the model better understand and represent important aspects related to Vietnam and its people. Finally, we also include a subset of Vietnamese from CulturaX and an additional 100B tokens in English. The final public dataset has a total of roughly 330 billion tokens. 3. 1. 2 In-house Data 
Influenced by the concept of synthetic textbooks for pretraining [GZA+23], our approach incorporates a mechanism that selects random text segments from a publicly available dataset. These segments are then integrated into over 80 bespoke prompt templates, each meticulously crafted to facilitate the rewriting task. The prompts, when fed into GPT-4, result in roughly 100,000 samples. These samples represent a high-quality synthetic dataset, specifically tailored for pretraining purposes.The methodology and workflow of this process are illustrated in Figure 2. Additionally, we employed Vietcuna-3B-v2, our successor smaller-scale language model, to train on the synthetic samples generated in Step 1. This training process utilized the rewriting task-specific prompts previously developed (as outlined in Step 2. 1), which is detailed in Figure 3. Subsequently, we replicated the procedure from Step 1 using this newly trained model. This iteration resulted in the generation of over 500 billion synthetic tokens, a process detailed in Step 2. 2. The final result of this process is morning 500 bilions of high quality Vietnamese tokens ready to be used to continue pretrain on the base LLaMA 2 with the expanded tokenizer. 3. 2 Supervised Instruction Fine-tuning 
In collaboration with Nous Research 3, we employed proprietary methodologies to create 500,000 Viet- namese synthetic samples. These samples encompassed both instructional and conversational formats. To enhance the dataset, an additional 500,000 English samples were sourced from the OpenHermes-2. 5 [tek23] and Capybara [DS23] datasets, also provided by Nous Research. This integration of datasets culminated in a comprehensive collection of 1 million samples, paving the way for the development of a bilingual language model. The comprehensive final dataset encompasses an array of tasks, including reasoning, role-playing, poem writing, coding, function calling, and agent prompting.This diversity ensures a broad scope of capabilities in the final model. We conducted a full fine-tuning of our pre- trained model over four epochs, which led to the creation of a state-of-the-art foundation language model specifically tailored for the Vietnamese language, more on the result in Section 4. 3. 3 VinaLLaMA-2. 7B 
Adopting the structured pruning procedure outlined in [XGZC23], we successfully reduced our model to a more compact variant with 2. 7 billion parameters. This process involved strategically pruning the network while retaining its core functional capabilities. Following the reduction in size, we applied the same supervised fine-tuning methodology to this smaller variant as was used for the 7B model, ensuring consistency in training approach across different model scales. 4 Evaluation 
In our research, we utilized three distinct evaluation benchmarks: VLSP, VMLU, and the Vicuna Benchmark, with the latter being translated into Vietnamese by VinAI Research. For each of these benchmarks, we implemented a dual-evaluation approach. Specifically, we conducted separate exper- iments for two categories of models: those in their pre-trained state and those that had undergone instructional fine-tuning. This methodology allowed us to compare the baseline capabilities of the pre-trained models against their performance post-instructional fine-tuning, providing insights into the effectiveness of fine-tuning in enhancing model proficiency within the context of these Vietnamese language benchmarks. For the pre-training benchmarks, we selected VLSP’s hoa-7b, BLOOM-7B, and PhoGPT-7B5 as the base models. In contrast, the instructional fine-tuned benchmarks comprised BLOOMZ-7B, Vietcuna- 7B-v3, PhoGPT-7B5-Instruct, URA-LLaMA-13B and SeaLLM-7B-chat. This diverse set of models provided a comprehensive perspective, enabling us to assess both the inherent capabilities of these models in their original form and their enhanced performance post-fine-tuning, specifically tailored to the Vietnamese language benchmarks.4. 1 VLSP 
The first benchmark employed is the VLSP-LLM 2023 [CHC+23]. This benchmark parallels the Open- LLM Leaderboard by HuggingFace, tailored specifically for the Vietnamese language. It encompasses four distinct assessments: ARC Challenge, HellaSwag, MMLU, and TruthfulQA, each meticulously adapted to Vietnamese. This comprehensive benchmark suite allows for a robust evaluation of lan- guage models in understanding and generating Vietnamese text across various domains and complex- ities. The results are reported in Table 1 and Table 2. In the pretrained segment of our study, the VinaLLaMA-7B model demonstrated superior performance compared to other open-source Large Lan- guage Models (LLMs) that support Vietnamese. This was evident in its outperformance on three of the four benchmarks, leading to it achieving state-of-the-art results based on the average score across these benchmarks. This signifies the robustness and effectiveness of the VinaLLaMA-7B model in handling a variety of tasks in the Vietnamese language context. In the supervised fine-tuning domain, our results continued to reflect a high level of excellence. No- tably, VinaLLaMA-7B-chat, comparable in scale, astonishingly outperformed larger models, including those with 13 billion parameters, in terms of average score. This impressive achievement highlights the efficacy of the fine-tuning process and the model’s adeptness at leveraging its training to deliver outstanding performance, even against much larger counterparts. Adding to this, the VinaLLaMA- 2. 7B, a significantly smaller model, showcased remarkable performance. It not only competed closely with larger 7B variants but also exceeded PhoGPT-7B5-Instruct in performance, while achieving in- ferencing speeds 60% faster.This showcases the model’s optimized balance between size, speed, and performance. The success of VinaLLaMA-7B-chat, and the noteworthy performance of VinaLLaMA- 2. 7B, underscore the considerable potential of well-implemented fine-tuning strategies with synthetic data in elevating the capabilities of Large Language Models, especially in contexts requiring specialized language processing. 4. 2 VMLU 
VMLU [AoST23], a benchmark suite tailored for evaluating foundation models in the Vietnamese language, comprises 10,880 multiple-choice questions across 58 subjects in domains like STEM, Hu- manities, and Social Sciences. Its diverse range of difficulty levels tests models from basic knowledge to advanced problem-solving. We selected VMLU for benchmarking due to its comprehensive coverage of Vietnam-related questions, providing a relevant and challenging environment for assessing model ca- pabilities in a context-specific manner. We conducted our experiments on the validation set of VMLU since the answers to test set are not publicly available, URA-LLaMA-13B is also not being test due to the lack of availability at testing time. We reported the results under two few-shot settings: 0-shot and 5-shot, which can be viewed in Table 3 and 4. 4. 3 Vicuna Benchmark Vietnamese 
The Vicuna Benchmark [ZCS+23], translated into Vietnamese by VinAI, serves as our final bench- mark. This comprehensive benchmark is composed of 80 distinct instructions spanning 9 diverse areas, providing a broad spectrum for assessing model capabilities. Uniquely, the evaluation of the results from all participating models is conducted using GPT-4, which introduces an innovative approach to performance assessment. This methodology employs an ELO ranking system, traditionally used in chess and other competitive games, to rate the models. Such a system offers a dynamic and rela- tive measure of model performance, allowing for a nuanced and comparative analysis of each model’s proficiency in handling a variety of tasks and instructions within the benchmark.This ELO-based evaluation provides a clear and quantifiable ranking of the models, reflecting their effectiveness and adaptability in the context of the Vietnamese language and the specific challenges presented by the Vicuna Benchmark. In our Vicuna Benchmark evaluation, responses from models were assessed using a detailed five- point scale: 0 (’very bad’), 1 (’bad’), 2 (’ok’), 3 (’good’), and 4 (’very good’). This granular scoring system allows for an in-depth evaluation of the quality of each model’s response. The final ELO score for each model is computed by aggregating these individual ratings, providing a holistic measure of a model’s overall performance across the benchmark’s varied tasks. To ensure language relevance, we implemented a strict rule: any response not in Vietnamese is automatically assigned a score of 0. This criterion underscores the importance of language-specific accuracy in our evaluation. The collective results, reflecting model performances across different tasks, are visually represented in Figure 4. In the interest of transparency and further research, we have made VinaLLaMA’s responses and our evaluation code publicly available 4 5. This not only allows for independent verification of our results but also facilitates further advancements in the field by providing valuable resources to other researchers. The benchmark results revealed that VinaLLaMA showcased commendable performance in Viet- namese, closely trailing behind ChatGPT-3. 5-Turbo in some benchmarks. This indicates that VinaLLaMA is highly effective in Vietnamese language tasks, with only a slight margin separating it from the more advanced ChatGPT-3. 5-Turbo, as shown in Table 5 and Table 6. PhoGPT also demonstrated better outcomes compared to other Vietnamese benchmarks, yet it still fell significantly behind in some areas, highlighting potential avenues for improvement. In contrast, URA-LLaMA-7B and 13B models performed less effectively, with some of their re- sponses not aligning with Vietnamese language requirements.This points to a critical need for targeted enhancements or language-specific training for these models to better cater to Vietnamese language processing. 5 Acknowledgement 
We are also immensely grateful to the teams at Nous Research, LAION, FPT Software AI Center, and Symato Team. Their assistance in the early stages of VinaLLaMA’s evaluation was crucial, providing insights that guided further improvements and enhancements of the model. We also want to send our appreciation to Nam Pham and Nhan Nguyen for the contribution in our public dataset. Lastly, our profound appreciation is extended to Google Cloud and Stability. AI. Their generous computational support was a cornerstone in bringing VinaLLaMA to fruition, enabling the intensive training and development processes required for such a sophisticated large language model. Their contribution was vital in transforming our vision for VinaLLaMA into a reality. 6 Conclusion 
In conclusion, the development of VinaLLaMA marks a significant milestone in the realm of language models, particularly in the context of Vietnamese language processing. Achieving state-of-the-art (SOTA) scores across all Vietnamese benchmarks, VinaLLaMA has demonstrated exceptional profi- ciency and adaptability. While its performance in English benchmarks was slightly less dominant, it still showed considerable competence, underscoring its effectiveness as a bilingual model. A key factor in VinaLLaMA’s success is the strategic use of carefully crafted synthetic data in its training regimen. This approach has proven to be highly effective, yielding impressive results and highlighting the importance of diverse and well-designed training datasets in the development of robust language models. VinaLLaMA’s achievements not only set a new standard for language models in Vietnamese but also contribute valuable insights into the broader field of natural language processing. It exemplifies how meticulous data preparation and comprehensive training can significantly enhance the capabilities of language models, paving the way for future advancements in this dynamic and rapidly evolving domain.
