{"cells":[{"cell_type":"markdown","metadata":{"id":"ivHdTOyFH0nT"},"source":["# OpenAI Function Calling 101\n","M·ªôt trong nh·ªØng kh√≥ khƒÉn khi s·ª≠ d·ª•ng c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) nh∆∞ ChatGPT l√† ch√∫ng kh√¥ng t·∫°o ra ƒë·∫ßu ra d·ªØ li·ªáu c√≥ c·∫•u tr√∫c. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªëi v·ªõi c√°c h·ªá th·ªëng l·∫≠p tr√¨nh ph·ª• thu·ªôc ph·∫ßn l·ªõn v√†o d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·ªÉ t∆∞∆°ng t√°c. V√≠ d·ª•, n·∫øu b·∫°n mu·ªën x√¢y d·ª±ng m·ªôt ch∆∞∆°ng tr√¨nh ph√¢n t√≠ch c·∫£m x√∫c c·ªßa m·ªôt b√†i ƒë√°nh gi√° phim, b·∫°n c√≥ th·ªÉ ph·∫£i th·ª±c hi·ªán m·ªôt ƒëo·∫°n code tr√¥ng gi·ªëng nh∆∞ sau:\n","\n","```\n","prompt = f'''\n","Please perform a sentiment analysis on the following movie review:\n","{MOVIE_REVIEW_TEXT}\n","Please output your response as a single word: either \"Positive\" or \"Negative\". Do not add any extra characters.\n","'''\n","```\n","\n","V·∫•n ƒë·ªÅ l√† ƒëi·ªÅu n√†y kh√¥ng ph·∫£i l√∫c n√†o c≈©ng hi·ªáu qu·∫£. Kh√° ph·ªï bi·∫øn l√† LLM s·∫Ω th√™m v√†o m·ªôt d·∫•u ch·∫•m kh√¥ng mong mu·ªën ho·∫∑c gi·∫£i th√≠ch d√†i h∆°n nh∆∞: \"C·∫£m x√∫c c·ªßa b·ªô phim n√†y l√†: T√≠ch c·ª±c.\" M·∫∑c d√π b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy (regex) ƒë·ªÉ l·∫•y ra c√¢u tr·∫£ l·ªùi (ü§¢), nh∆∞ng r√µ r√†ng ƒë√¢y kh√¥ng ph·∫£i l√† l√Ω t∆∞·ªüng. ƒêi·ªÅu l√Ω t∆∞·ªüng s·∫Ω l√† n·∫øu LLM tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng c·∫•u tr√∫c JSON nh∆∞ sau:\n","\n","```\n","{\n","    'sentiment': 'positive'\n","}\n","```\n","\n","OpenAI ƒë√£ gi·ªõi thi·ªáu m·ªôt t√≠nh nƒÉng m·ªõi g·ªçi l√† function calling, gi√∫p gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ tr√™n. Function calling ch√≠nh l√† c√¢u tr·∫£ l·ªùi cho v·∫•n ƒë·ªÅ tr√™n. Jupyter notebook n√†y s·∫Ω minh h·ªça m·ªôt v√≠ d·ª• ƒë∆°n gi·∫£n v·ªÅ c√°ch s·ª≠ d·ª•ng function calling m·ªõi c·ªßa OpenAI trong Python. N·∫øu b·∫°n mu·ªën xem t√†i li·ªáu ƒë·∫ßy ƒë·ªß, [vui l√≤ng xem li√™n k·∫øt n√†y](https://platform.openai.com/docs/guides/gpt/function-calling)."]},{"cell_type":"markdown","metadata":{"id":"cT1ePnTaH0nV"},"source":["## Notebook Setup\n","H√£y b·∫Øt ƒë·∫ßu v·ªõi c√°c import. B√¢y gi·ªù, c√≥ th·ªÉ b·∫°n ƒë√£ c√†i ƒë·∫∑t client Python `openai`, nh∆∞ng r·∫•t c√≥ th·ªÉ b·∫°n c·∫ßn n√¢ng c·∫•p n√≥ ƒë·ªÉ c√≥ ƒë∆∞·ª£c ch·ª©c nƒÉng function calling m·ªõi. ƒê√¢y l√† c√°ch n√¢ng c·∫•p trong Terminal / Powershell c·ªßa b·∫°n b·∫±ng `pip`:\n","\n","```\n","pip install openai --upgrade\n","```"]},{"cell_type":"code","source":["!pip install groq --upgrade"],"metadata":{"id":"iN4qCK21hBor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728856965203,"user_tz":-420,"elapsed":20779,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"e748c3f2-389b-49d4-87aa-a0a4821ed58d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n","Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n","Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzJqI_a6H0nV"},"outputs":[],"source":["# Importing the necessary Python libraries\n","from IPython.display import clear_output\n","import os\n","import json\n","import yaml\n","from groq import Groq"]},{"cell_type":"markdown","metadata":{"id":"URyKfrF1H0nW"},"source":["ƒê·ªÉ ki·ªÉm tra ch·ª©c nƒÉng function calling, t√¥i ƒë√£ vi·∫øt m·ªôt ƒëo·∫°n \"About Me\" ng·∫Øn ch·ª©a c√°c s·ª± th·∫≠t c·ª• th·ªÉ m√† ch√∫ng ta s·∫Ω ph√¢n t√≠ch th√†nh c√°c c·∫•u tr√∫c d·ªØ li·ªáu ph√π h·ª£p, bao g·ªìm s·ªë nguy√™n v√† chu·ªói. H√£y t·∫£i vƒÉn b·∫£n n√†y v√†o"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHNSQ593H0nW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857123781,"user_tz":-420,"elapsed":331,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"176523ad-a357-401b-8d7b-fa0e7e0c91f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\n"]}],"source":["from google.colab import userdata\n","\n","\n","# Loading the \"About Me\" text from local file\n","os.environ['GROQ_API_KEY'] = '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'\n","about_me = \"Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\"\n","\n","print(about_me)\n"]},{"cell_type":"markdown","metadata":{"id":"4W4Q38BcH0nX"},"source":["## The Pre-Function Calling Days\n","Tr∆∞·ªõc khi ch√∫ng ta minh h·ªça function calling, h√£y minh h·ªça c√°ch ch√∫ng ta ƒë√£ s·ª≠ d·ª•ng prompt engineering v√† Regex ƒë·ªÉ t·∫°o ra m·ªôt JSON c√≥ c·∫•u tr√∫c m√† ch√∫ng ta c√≥ th·ªÉ l√†m vi·ªác m·ªôt c√°ch l·∫≠p tr√¨nh sau n√†y."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7u7WOlAH0nX"},"outputs":[],"source":["# Engineering a prompt to extract as much information from \"About Me\" as a JSON object\n","about_me_prompt = f'''\n","Please extract information as a JSON object. Please look for the following pieces of information.\n","Name\n","Job title\n","Company\n","Number of children as a single number\n","Car make\n","Car model\n","Favorite video game series\n","\n","This is the body of text to extract the information from:\n","{about_me}\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ276CVDH0nX"},"outputs":[],"source":["client = Groq(\n","    api_key = os.environ.get('GROQ_API_KEY')\n",")\n","\n","groq_response = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': about_me_prompt}]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXT-v6-bH0nX","outputId":"a518e0b2-ec5c-443b-baf0-e73c9d10fb1a","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"error","timestamp":1728857139604,"user_tz":-420,"elapsed":348,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c0f48eb89778>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the response as a JSON object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroq_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}],"source":["# Loading the response as a JSON object\n","json_response = json.loads(groq_response.choices[0].message.content)\n","print(json_response)"]},{"cell_type":"code","source":["groq_response.choices[0].message.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"mEgTrob46oyp","executionInfo":{"status":"ok","timestamp":1728857260180,"user_tz":-420,"elapsed":370,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"9ba915ee-97ad-470d-cf47-0b60ccd2d0b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Here is the extracted information in a JSON object:\\n\\n```\\n{\\n  \"name\": \"David Hundley\",\\n  \"jobTitle\": \"Principal Machine Learning Engineer\",\\n  \"company\": \"State Farm\",\\n  \"children\": 2,\\n  \"carMake\": \"Tesla\",\\n  \"carModel\": \"Model 3\",\\n  \"favoriteVideoGameSeries\": \"The Legend of Zelda\"\\n}\\n```\\n\\nLet me know if you need anything else!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"KhjGXw7QH0nX"},"source":["## Using the New Function Calling Capabilities\n","B√¢y gi·ªù ch√∫ng ta ƒë√£ minh h·ªça c√°ch ch√∫ng ta ƒë√£ t·ª´ng l·∫•y ƒë∆∞·ª£c JSON c√≥ c·∫•u tr√∫c trong nh·ªØng ng√†y tr∆∞·ªõc khi c√≥ function calling, h√£y chuy·ªÉn sang c√°ch ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng function calling ƒë·ªÉ tr√≠ch xu·∫•t c√°c k·∫øt qu·∫£ t∆∞∆°ng t·ª± nh∆∞ng theo c√°ch nh·∫•t qu√°n h∆°n cho vi·ªác s·ª≠ d·ª•ng h·ªá th·ªëng c·ªßa ch√∫ng ta. Ch√∫ng ta s·∫Ω b·∫Øt ƒë·∫ßu ƒë∆°n gi·∫£n h∆°n v·ªõi m·ªôt h√†m t√πy ch·ªânh duy nh·∫•t v√† sau ƒë√≥ gi·∫£i quy·∫øt m·ªôt s·ªë ch·ª©c nƒÉng \"n√¢ng cao\" h∆°n."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWikJtaqH0nX"},"outputs":[],"source":["# Defining our initial extract_person_info function\n","def extract_person_info(name, job_title, num_children):\n","    '''\n","    Prints basic \"About Me\" information\n","\n","    Inputs:\n","        name (str): Name of the person\n","        job_title (str): Job title of the person\n","        num_chilren (int): The number of children the parent has.\n","    '''\n","\n","    print(f'This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNS7-6XqH0nX"},"outputs":[],"source":["# Defining how we want ChatGPT to call our custom functions\n","my_custom_functions = [\n","    {\n","        'name': 'extract_person_info',\n","        'description': 'Get \"About Me\" information from the body of the input text',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                }\n","            }\n","        }\n","    }\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVKEN9tQH0nY","outputId":"18048ae4-b91a-4d70-dcbc-14476ba4597e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857394558,"user_tz":-420,"elapsed":655,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ChatCompletion(id='chatcmpl-ac6ef877-7808-4ede-bbf5-05a0b031e887', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857394, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_af05557ca2', usage=CompletionUsage(completion_tokens=88, prompt_tokens=1021, total_tokens=1109, completion_time=0.073333333, prompt_time=0.115240994, queue_time=0.002358924999999998, total_time=0.188574327), x_groq={'id': 'req_01ja3ywjvdf55am0tb21q4gbxc'})\n"]}],"source":["# Getting the response back from ChatGPT (gpt-3.5-turbo)\n","groq_response = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': about_me}],\n","    functions = my_custom_functions,\n","    function_call = 'auto'\n",")\n","\n","print(groq_response)"]},{"cell_type":"code","source":["print(groq_response.choices[0].message.function_call)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceI9oBI7iQWE","executionInfo":{"status":"ok","timestamp":1728857423184,"user_tz":-420,"elapsed":335,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"f90abcf9-8c82-4811-baf3-d1efd5397b28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info')\n"]}]},{"cell_type":"markdown","metadata":{"id":"f7kQ2GsUH0nY"},"source":["### What if the prompt I submit doesn't contain the information I want to extract per my custom function?\n","Copy code\n","Trong v√≠ d·ª• ban ƒë·∫ßu c·ªßa ch√∫ng ta, h√†m t√πy ch·ªânh ƒë√£ t√¨m c√°ch tr√≠ch xu·∫•t ba th√¥ng tin r·∫•t c·ª• th·ªÉ v√† ch√∫ng ta ƒë√£ ch·ª©ng minh r·∫±ng ƒëi·ªÅu n√†y ho·∫°t ƒë·ªông th√†nh c√¥ng b·∫±ng c√°ch truy·ªÅn vƒÉn b·∫£n \"About Me\" t√πy ch·ªânh c·ªßa t√¥i d∆∞·ªõi d·∫°ng m·ªôt prompt. Nh∆∞ng b·∫°n c√≥ th·ªÉ t·ª± h·ªèi, ƒëi·ªÅu g√¨ s·∫Ω x·∫£y ra n·∫øu b·∫°n truy·ªÅn v√†o b·∫•t k·ª≥ prompt n√†o kh√°c kh√¥ng ch·ª©a th√¥ng tin ƒë√≥?\n","\n","H√£y nh·ªõ l·∫°i r·∫±ng ch√∫ng ta ƒë√£ ƒë·∫∑t m·ªôt tham s·ªë trong l·ªánh g·ªçi API client c·ªßa m√¨nh g·ªçi l√† function_call m√† ch√∫ng ta ƒë·∫∑t th√†nh auto. Ch√∫ng ta s·∫Ω kh√°m ph√° ƒëi·ªÅu n√†y s√¢u h∆°n n·ªØa trong ph·∫ßn ti·∫øp theo, nh∆∞ng v·ªÅ c∆° b·∫£n, tham s·ªë n√†y ƒëang y√™u c·∫ßu ChatGPT s·ª≠ d·ª•ng ph√°n ƒëo√°n t·ªët nh·∫•t c·ªßa n√≥ ƒë·ªÉ t√¨m ra khi n√†o c·∫ßn c·∫•u tr√∫c ƒë·∫ßu ra cho m·ªôt trong c√°c h√†m t√πy ch·ªânh c·ªßa ch√∫ng ta.\n","\n","V·∫≠y ƒëi·ªÅu g√¨ s·∫Ω x·∫£y ra khi ch√∫ng ta g·ª≠i m·ªôt prompt kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ h√†m t√πy ch·ªânh n√†o c·ªßa ch√∫ng ta? N√≥i m·ªôt c√°ch ƒë∆°n gi·∫£n, n√≥ s·∫Ω m·∫∑c ƒë·ªãnh tr·ªü l·∫°i h√†nh vi th√¥ng th∆∞·ªùng nh∆∞ th·ªÉ function calling kh√¥ng t·ªìn t·∫°i. H√£y ki·ªÉm tra ƒëi·ªÅu n√†y v·ªõi m·ªôt prompt t√πy √Ω: \"Th√°p Eiffel cao bao nhi√™u?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1UBGLPMH0nY","outputId":"d0e35f96-05ac-4694-88a4-26e1a522e280","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1728857816963,"user_tz":-420,"elapsed":1087,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"error","ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"questions\": {\\n\\t\\t\\t\\t\\t\"text\": \"What is the height of the Eiffel Tower in meters?\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-7233a23ad8a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m groq_response3 = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'llama3-8b-8192'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'How tall is the Eiffel Tower?'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_custom_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfunction_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"questions\": {\\n\\t\\t\\t\\t\\t\"text\": \"What is the height of the Eiffel Tower in meters?\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}"]}],"source":["\n","groq_response3 = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': 'How tall is the Eiffel Tower?'}],\n","    functions = my_custom_functions,\n","    function_call = 'auto'\n",")\n","\n","print(groq_response3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-BK6U8SH0nY"},"outputs":[],"source":["# Defining a function to extract only vehicle information\n","def extract_vehicle_info(vehicle_make, vehicle_model):\n","    '''\n","    Prints basic vehicle information\n","\n","    Inputs:\n","        - vehicle_make (str): Make of the vehicle\n","        - vehicle_model (str): Model of the vehicle\n","    '''\n","\n","    print(f'Vehicle make: {vehicle_make}\\nVehicle model: {vehicle_model}')\n","\n","\n","\n","# Defining a function to extract all information provided in the original \"About Me\" prompt\n","def extract_all_info(name, job_title, num_children, vehicle_make, vehicle_model, company_name, favorite_vg_series):\n","    '''\n","    Prints the full \"About Me\" information\n","\n","    Inputs:\n","        - name (str): Name of the person\n","        - job_title (str): Job title of the person\n","        - num_chilren (int): The number of children the parent has\n","        - vehicle_make (str): Make of the vehicle\n","        - vehicle_model (str): Model of the vehicle\n","        - company_name (str): Name of the company the person works for\n","        - favorite_vg_series (str): Person's favorite video game series.\n","    '''\n","\n","    print(f'''\n","    This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.\n","    They drive a {vehicle_make} {vehicle_model}.\n","    They work for {company_name}.\n","    Their favorite video game series is {favorite_vg_series}.\n","    ''')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MCTQFNfH0nY"},"outputs":[],"source":["# Defining how we want ChatGPT to call our custom functions\n","my_custom_functions = [\n","    {\n","        'name': 'extract_person_info',\n","        'description': 'Get \"About Me\" information from the body of the input text',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                }\n","            }\n","        }\n","    },\n","    {\n","        'name': 'extract_vehicle_info',\n","        'description': 'Extract the make and model of the person\\'s car',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'vehicle_make': {\n","                    'type': 'string',\n","                    'description': 'Make of the person\\'s vehicle'\n","                },\n","                'vehicle_model': {\n","                    'type': 'string',\n","                    'description': 'Model of the person\\'s vehicle'\n","                }\n","            }\n","        }\n","    },\n","    {\n","        'name': 'extract_all_info',\n","        'description': 'Extract all information about a person including their vehicle make and model',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                },\n","                'vehicle_make': {\n","                    'type': 'string',\n","                    'description': 'Make of the person\\'s vehicle'\n","                },\n","                'vehicle_model': {\n","                    'type': 'string',\n","                    'description': 'Model of the person\\'s vehicle'\n","                },\n","                'company_name': {\n","                    'type': 'string',\n","                    'description': 'Name of the company the person works for'\n","                },\n","                'favorite_vg_series': {\n","                    'type': 'string',\n","                    'description': 'Name of the person\\'s favorite video game series'\n","                }\n","            }\n","        }\n","    }\n","]"]},{"cell_type":"markdown","metadata":{"id":"p7bKZqEnH0nY"},"source":["Now let's demonstrate what happens when we apply 3 different samples to all of the custom functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ondbhtOmH0nY"},"outputs":[],"source":["# Defining a list of samples\n","samples = [\n","    about_me,\n","    'My name is David Hundley. I am a principal machine learning engineer, and I have two daughters.',\n","    'She drives a Kia Sportage.'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNF0ss92H0nY","outputId":"20ba27db-b108-4cf6-f3f1-a65fa59402ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857771812,"user_tz":-420,"elapsed":1832,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample #1's results:\n","ChatCompletion(id='chatcmpl-94b63aa5-7350-4225-a3d2-7ff1fa501743', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857770, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_6a6771ae9c', usage=CompletionUsage(completion_tokens=77, prompt_tokens=1429, total_tokens=1506, completion_time=0.064166667, prompt_time=0.203900812, queue_time=0.0021255980000000063, total_time=0.268067479), x_groq={'id': 'req_01ja3z826ge6vsjkx56hmqx5a7'})\n","Sample #2's results:\n","ChatCompletion(id='chatcmpl-54e72624-a4db-46f3-a0d0-ae356ab5aaee', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857770, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_af05557ca2', usage=CompletionUsage(completion_tokens=77, prompt_tokens=1391, total_tokens=1468, completion_time=0.064166667, prompt_time=0.283461586, queue_time=0.003471945000000032, total_time=0.347628253), x_groq={'id': 'req_01ja3z82rsedzsybq8g5wft0e9'})\n","Sample #3's results:\n","ChatCompletion(id='chatcmpl-9eb834f6-d2b0-4a0b-8def-cf6cbdc0d3ca', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"vehicle_make\":\"Kia\"}', name='extract_vehicle_info'), tool_calls=None))], created=1728857771, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_179b0f92c9', usage=CompletionUsage(completion_tokens=76, prompt_tokens=1377, total_tokens=1453, completion_time=0.063333333, prompt_time=0.189629938, queue_time=0.002821443000000007, total_time=0.252963271), x_groq={'id': 'req_01ja3z83anf5wvhw1vg5kb47fj'})\n"]}],"source":["# Iterating over the three samples\n","for i, sample in enumerate(samples):\n","\n","    print(f'Sample #{i + 1}\\'s results:')\n","\n","    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n","    groq_response1 = client.chat.completions.create(\n","        model = 'llama3-8b-8192',\n","        messages = [{'role': 'user', 'content': sample}],\n","        functions = my_custom_functions,\n","        function_call = 'auto'\n","    )\n","\n","    # Printing the sample's response\n","    print(groq_response1)"]},{"cell_type":"markdown","metadata":{"id":"QD7qtImGH0nY"},"source":["With each of the respective prompts, ChatGPT selected the correct custom function, and we can specifically note that in the `name` value under `function_call` in the API's response object. In addition to this being a handy way to identify which function to use the arguments for, we can programmatically map our actual custom Python function to this value to run the correct code appropriately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGfDFvrTH0nZ","outputId":"1a0a319f-4b34-496b-c2c2-43c3d0032640","colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"error","timestamp":1728857908204,"user_tz":-420,"elapsed":1950,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample #1's results:\n","This person's name is David Hundley. Their job title is principal machine learning engineer, and they have 2 children.\n","Sample #2's results:\n","This person's name is Principal Machine Learning Engineer. Their job title is David Hundley, and they have 2 children.\n","Sample #3's results:\n"]},{"output_type":"error","ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_person_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"num_children\": \"0\",\\n\\t\\t\\t\\t\"name\": \"Unknown\",\\n\\t\\t\\t\\t\"job_title\": \"Unknown\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-02e0bb8931f5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Getting the response back from ChatGPT (gpt-3.5-turbo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     openai_response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'llama3-8b-8192'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_person_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"num_children\": \"0\",\\n\\t\\t\\t\\t\"name\": \"Unknown\",\\n\\t\\t\\t\\t\"job_title\": \"Unknown\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}"]}],"source":["# Iterating over the three samples\n","for i, sample in enumerate(samples):\n","\n","    print(f'Sample #{i + 1}\\'s results:')\n","\n","    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n","    openai_response = client.chat.completions.create(\n","        model = 'llama3-8b-8192',\n","        messages = [{'role': 'user', 'content': sample}],\n","        functions = my_custom_functions,\n","        function_call = 'auto'\n","    ).choices[0].message\n","\n","    # Checking to see that a function call was invoked\n","    if openai_response.function_call:\n","        # Checking to see which specific function call was invoked\n","        function_called = openai_response.function_call.name\n","\n","        # Extracting the arguments of the function call\n","        function_args = json.loads(openai_response.function_call.arguments)\n","\n","        # Invoking the proper functions\n","        if function_called == 'extract_person_info':\n","            extract_person_info(*list(function_args.values()))\n","        elif function_called == 'extract_vehicle_info':\n","            extract_vehicle_info(*list(function_args.values()))\n","        elif function_called == 'extract_all_info':\n","            extract_all_info(*list(function_args.values()))"]},{"cell_type":"markdown","metadata":{"id":"NpXnf2mAH0nZ"},"source":["## OpenAI Function Calling with LangChain\n","Given its popularity amongst the Generative AI community, I thought I'd re-visit this notebook and add some code to show how you might make use of this exact same functionality in LangChain"]},{"cell_type":"code","source":["!pip install --quiet -U langchain langchain-groq langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhOmubQskrv6","executionInfo":{"status":"ok","timestamp":1728857999926,"user_tz":-420,"elapsed":9005,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"7c4866eb-8a48-40c7-e15a-bd5e15dc7b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/2.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/49.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zivmk4LtH0nZ"},"outputs":[],"source":["# Importing the LangChain objects\n","from langchain_groq import ChatGroq\n","from langchain.chains import LLMChain\n","from langchain.prompts.chat import ChatPromptTemplate\n","from langchain.chains.openai_functions import create_structured_output_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cp067ipkH0nZ"},"outputs":[],"source":["# Setting the proper instance of the OpenAI model\n","llm = ChatGroq(model = 'llama3-8b-8192')\n","\n","# Setting a LangChain ChatPromptTemplate\n","chat_prompt_template = ChatPromptTemplate.from_template('{my_prompt}')\n","\n","# Setting the JSON schema for extracting vehicle information\n","langchain_json_schema = {\n","    'name': 'extract_vehicle_info',\n","    'description': 'Extract the make and model of the person\\'s car',\n","    'type': 'object',\n","    'properties': {\n","        'vehicle_make': {\n","            'title': 'Vehicle Make',\n","            'type': 'string',\n","            'description': 'Make of the person\\'s vehicle'\n","        },\n","        'vehicle_model': {\n","            'title': 'Vehicle Model',\n","            'type': 'string',\n","            'description': 'Model of the person\\'s vehicle'\n","        }\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLkSz-PUH0nZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728858144132,"user_tz":-420,"elapsed":339,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}},"outputId":"431dc401-b0c6-4de8-8c2a-31518ab35568"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-0d6661282edd>:2: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 1.0. Use :meth:`~ChatOpenAI.with_structured_output` instead.\n","  chain = create_structured_output_chain(output_schema = langchain_json_schema,\n"]}],"source":["# Defining the LangChain chain object for function calling\n","chain = create_structured_output_chain(output_schema = langchain_json_schema,\n","                                       llm = llm,\n","                                       prompt = chat_prompt_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lux3-94PH0nZ","outputId":"daab3fe7-044a-410b-e9cc-801f9440d8c7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728858158751,"user_tz":-420,"elapsed":667,"user":{"displayName":"H·∫£o Nguy·ªÖn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'my_prompt': 'I drive a Tesla Model 3', 'function': {'extract_vehicle_info': {'vehicle_make': 'Tesla', 'vehicle_model': 'Model 3'}}}\n"]}],"source":["# Getting results with a demo prompt\n","print(chain.invoke(input = {'my_prompt': 'I drive a Tesla Model 3'}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TfxGNd9H0nZ"},"outputs":[],"source":["clear_output()"]},{"cell_type":"code","source":[],"metadata":{"id":"7svqhafB0Zzx"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1sJQ9--ciLzUDacRFbHAYtyprlHf0Dauv","timestamp":1728856804408}]}},"nbformat":4,"nbformat_minor":0}