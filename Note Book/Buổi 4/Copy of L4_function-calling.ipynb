{"cells":[{"cell_type":"markdown","metadata":{"id":"ivHdTOyFH0nT"},"source":["# OpenAI Function Calling 101\n","Một trong những khó khăn khi sử dụng các mô hình ngôn ngữ lớn (LLM) như ChatGPT là chúng không tạo ra đầu ra dữ liệu có cấu trúc. Điều này rất quan trọng đối với các hệ thống lập trình phụ thuộc phần lớn vào dữ liệu có cấu trúc để tương tác. Ví dụ, nếu bạn muốn xây dựng một chương trình phân tích cảm xúc của một bài đánh giá phim, bạn có thể phải thực hiện một đoạn code trông giống như sau:\n","\n","```\n","prompt = f'''\n","Please perform a sentiment analysis on the following movie review:\n","{MOVIE_REVIEW_TEXT}\n","Please output your response as a single word: either \"Positive\" or \"Negative\". Do not add any extra characters.\n","'''\n","```\n","\n","Vấn đề là điều này không phải lúc nào cũng hiệu quả. Khá phổ biến là LLM sẽ thêm vào một dấu chấm không mong muốn hoặc giải thích dài hơn như: \"Cảm xúc của bộ phim này là: Tích cực.\" Mặc dù bạn có thể sử dụng biểu thức chính quy (regex) để lấy ra câu trả lời (🤢), nhưng rõ ràng đây không phải là lý tưởng. Điều lý tưởng sẽ là nếu LLM trả về kết quả dưới dạng cấu trúc JSON như sau:\n","\n","```\n","{\n","    'sentiment': 'positive'\n","}\n","```\n","\n","OpenAI đã giới thiệu một tính năng mới gọi là function calling, giúp giải quyết vấn đề trên. Function calling chính là câu trả lời cho vấn đề trên. Jupyter notebook này sẽ minh họa một ví dụ đơn giản về cách sử dụng function calling mới của OpenAI trong Python. Nếu bạn muốn xem tài liệu đầy đủ, [vui lòng xem liên kết này](https://platform.openai.com/docs/guides/gpt/function-calling)."]},{"cell_type":"markdown","metadata":{"id":"cT1ePnTaH0nV"},"source":["## Notebook Setup\n","Hãy bắt đầu với các import. Bây giờ, có thể bạn đã cài đặt client Python `openai`, nhưng rất có thể bạn cần nâng cấp nó để có được chức năng function calling mới. Đây là cách nâng cấp trong Terminal / Powershell của bạn bằng `pip`:\n","\n","```\n","pip install openai --upgrade\n","```"]},{"cell_type":"code","source":["!pip install groq --upgrade"],"metadata":{"id":"iN4qCK21hBor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728856965203,"user_tz":-420,"elapsed":20779,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"e748c3f2-389b-49d4-87aa-a0a4821ed58d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n","Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n","Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzJqI_a6H0nV"},"outputs":[],"source":["# Importing the necessary Python libraries\n","from IPython.display import clear_output\n","import os\n","import json\n","import yaml\n","from groq import Groq"]},{"cell_type":"markdown","metadata":{"id":"URyKfrF1H0nW"},"source":["Để kiểm tra chức năng function calling, tôi đã viết một đoạn \"About Me\" ngắn chứa các sự thật cụ thể mà chúng ta sẽ phân tích thành các cấu trúc dữ liệu phù hợp, bao gồm số nguyên và chuỗi. Hãy tải văn bản này vào"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHNSQ593H0nW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857123781,"user_tz":-420,"elapsed":331,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"176523ad-a357-401b-8d7b-fa0e7e0c91f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\n"]}],"source":["from google.colab import userdata\n","\n","\n","# Loading the \"About Me\" text from local file\n","os.environ['GROQ_API_KEY'] = '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'\n","about_me = \"Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\"\n","\n","print(about_me)\n"]},{"cell_type":"markdown","metadata":{"id":"4W4Q38BcH0nX"},"source":["## The Pre-Function Calling Days\n","Trước khi chúng ta minh họa function calling, hãy minh họa cách chúng ta đã sử dụng prompt engineering và Regex để tạo ra một JSON có cấu trúc mà chúng ta có thể làm việc một cách lập trình sau này."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7u7WOlAH0nX"},"outputs":[],"source":["# Engineering a prompt to extract as much information from \"About Me\" as a JSON object\n","about_me_prompt = f'''\n","Please extract information as a JSON object. Please look for the following pieces of information.\n","Name\n","Job title\n","Company\n","Number of children as a single number\n","Car make\n","Car model\n","Favorite video game series\n","\n","This is the body of text to extract the information from:\n","{about_me}\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ276CVDH0nX"},"outputs":[],"source":["client = Groq(\n","    api_key = os.environ.get('GROQ_API_KEY')\n",")\n","\n","groq_response = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': about_me_prompt}]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXT-v6-bH0nX","outputId":"a518e0b2-ec5c-443b-baf0-e73c9d10fb1a","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"error","timestamp":1728857139604,"user_tz":-420,"elapsed":348,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c0f48eb89778>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the response as a JSON object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroq_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}],"source":["# Loading the response as a JSON object\n","json_response = json.loads(groq_response.choices[0].message.content)\n","print(json_response)"]},{"cell_type":"code","source":["groq_response.choices[0].message.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"mEgTrob46oyp","executionInfo":{"status":"ok","timestamp":1728857260180,"user_tz":-420,"elapsed":370,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"9ba915ee-97ad-470d-cf47-0b60ccd2d0b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Here is the extracted information in a JSON object:\\n\\n```\\n{\\n  \"name\": \"David Hundley\",\\n  \"jobTitle\": \"Principal Machine Learning Engineer\",\\n  \"company\": \"State Farm\",\\n  \"children\": 2,\\n  \"carMake\": \"Tesla\",\\n  \"carModel\": \"Model 3\",\\n  \"favoriteVideoGameSeries\": \"The Legend of Zelda\"\\n}\\n```\\n\\nLet me know if you need anything else!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"KhjGXw7QH0nX"},"source":["## Using the New Function Calling Capabilities\n","Bây giờ chúng ta đã minh họa cách chúng ta đã từng lấy được JSON có cấu trúc trong những ngày trước khi có function calling, hãy chuyển sang cách chúng ta có thể sử dụng function calling để trích xuất các kết quả tương tự nhưng theo cách nhất quán hơn cho việc sử dụng hệ thống của chúng ta. Chúng ta sẽ bắt đầu đơn giản hơn với một hàm tùy chỉnh duy nhất và sau đó giải quyết một số chức năng \"nâng cao\" hơn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWikJtaqH0nX"},"outputs":[],"source":["# Defining our initial extract_person_info function\n","def extract_person_info(name, job_title, num_children):\n","    '''\n","    Prints basic \"About Me\" information\n","\n","    Inputs:\n","        name (str): Name of the person\n","        job_title (str): Job title of the person\n","        num_chilren (int): The number of children the parent has.\n","    '''\n","\n","    print(f'This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNS7-6XqH0nX"},"outputs":[],"source":["# Defining how we want ChatGPT to call our custom functions\n","my_custom_functions = [\n","    {\n","        'name': 'extract_person_info',\n","        'description': 'Get \"About Me\" information from the body of the input text',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                }\n","            }\n","        }\n","    }\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVKEN9tQH0nY","outputId":"18048ae4-b91a-4d70-dcbc-14476ba4597e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857394558,"user_tz":-420,"elapsed":655,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ChatCompletion(id='chatcmpl-ac6ef877-7808-4ede-bbf5-05a0b031e887', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857394, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_af05557ca2', usage=CompletionUsage(completion_tokens=88, prompt_tokens=1021, total_tokens=1109, completion_time=0.073333333, prompt_time=0.115240994, queue_time=0.002358924999999998, total_time=0.188574327), x_groq={'id': 'req_01ja3ywjvdf55am0tb21q4gbxc'})\n"]}],"source":["# Getting the response back from ChatGPT (gpt-3.5-turbo)\n","groq_response = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': about_me}],\n","    functions = my_custom_functions,\n","    function_call = 'auto'\n",")\n","\n","print(groq_response)"]},{"cell_type":"code","source":["print(groq_response.choices[0].message.function_call)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceI9oBI7iQWE","executionInfo":{"status":"ok","timestamp":1728857423184,"user_tz":-420,"elapsed":335,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"f90abcf9-8c82-4811-baf3-d1efd5397b28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info')\n"]}]},{"cell_type":"markdown","metadata":{"id":"f7kQ2GsUH0nY"},"source":["### What if the prompt I submit doesn't contain the information I want to extract per my custom function?\n","Copy code\n","Trong ví dụ ban đầu của chúng ta, hàm tùy chỉnh đã tìm cách trích xuất ba thông tin rất cụ thể và chúng ta đã chứng minh rằng điều này hoạt động thành công bằng cách truyền văn bản \"About Me\" tùy chỉnh của tôi dưới dạng một prompt. Nhưng bạn có thể tự hỏi, điều gì sẽ xảy ra nếu bạn truyền vào bất kỳ prompt nào khác không chứa thông tin đó?\n","\n","Hãy nhớ lại rằng chúng ta đã đặt một tham số trong lệnh gọi API client của mình gọi là function_call mà chúng ta đặt thành auto. Chúng ta sẽ khám phá điều này sâu hơn nữa trong phần tiếp theo, nhưng về cơ bản, tham số này đang yêu cầu ChatGPT sử dụng phán đoán tốt nhất của nó để tìm ra khi nào cần cấu trúc đầu ra cho một trong các hàm tùy chỉnh của chúng ta.\n","\n","Vậy điều gì sẽ xảy ra khi chúng ta gửi một prompt không khớp với bất kỳ hàm tùy chỉnh nào của chúng ta? Nói một cách đơn giản, nó sẽ mặc định trở lại hành vi thông thường như thể function calling không tồn tại. Hãy kiểm tra điều này với một prompt tùy ý: \"Tháp Eiffel cao bao nhiêu?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1UBGLPMH0nY","outputId":"d0e35f96-05ac-4694-88a4-26e1a522e280","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1728857816963,"user_tz":-420,"elapsed":1087,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"error","ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"questions\": {\\n\\t\\t\\t\\t\\t\"text\": \"What is the height of the Eiffel Tower in meters?\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-7233a23ad8a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m groq_response3 = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'llama3-8b-8192'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'How tall is the Eiffel Tower?'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_custom_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfunction_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"questions\": {\\n\\t\\t\\t\\t\\t\"text\": \"What is the height of the Eiffel Tower in meters?\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}"]}],"source":["\n","groq_response3 = client.chat.completions.create(\n","    model = 'llama3-8b-8192',\n","    messages = [{'role': 'user', 'content': 'How tall is the Eiffel Tower?'}],\n","    functions = my_custom_functions,\n","    function_call = 'auto'\n",")\n","\n","print(groq_response3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-BK6U8SH0nY"},"outputs":[],"source":["# Defining a function to extract only vehicle information\n","def extract_vehicle_info(vehicle_make, vehicle_model):\n","    '''\n","    Prints basic vehicle information\n","\n","    Inputs:\n","        - vehicle_make (str): Make of the vehicle\n","        - vehicle_model (str): Model of the vehicle\n","    '''\n","\n","    print(f'Vehicle make: {vehicle_make}\\nVehicle model: {vehicle_model}')\n","\n","\n","\n","# Defining a function to extract all information provided in the original \"About Me\" prompt\n","def extract_all_info(name, job_title, num_children, vehicle_make, vehicle_model, company_name, favorite_vg_series):\n","    '''\n","    Prints the full \"About Me\" information\n","\n","    Inputs:\n","        - name (str): Name of the person\n","        - job_title (str): Job title of the person\n","        - num_chilren (int): The number of children the parent has\n","        - vehicle_make (str): Make of the vehicle\n","        - vehicle_model (str): Model of the vehicle\n","        - company_name (str): Name of the company the person works for\n","        - favorite_vg_series (str): Person's favorite video game series.\n","    '''\n","\n","    print(f'''\n","    This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.\n","    They drive a {vehicle_make} {vehicle_model}.\n","    They work for {company_name}.\n","    Their favorite video game series is {favorite_vg_series}.\n","    ''')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MCTQFNfH0nY"},"outputs":[],"source":["# Defining how we want ChatGPT to call our custom functions\n","my_custom_functions = [\n","    {\n","        'name': 'extract_person_info',\n","        'description': 'Get \"About Me\" information from the body of the input text',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                }\n","            }\n","        }\n","    },\n","    {\n","        'name': 'extract_vehicle_info',\n","        'description': 'Extract the make and model of the person\\'s car',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'vehicle_make': {\n","                    'type': 'string',\n","                    'description': 'Make of the person\\'s vehicle'\n","                },\n","                'vehicle_model': {\n","                    'type': 'string',\n","                    'description': 'Model of the person\\'s vehicle'\n","                }\n","            }\n","        }\n","    },\n","    {\n","        'name': 'extract_all_info',\n","        'description': 'Extract all information about a person including their vehicle make and model',\n","        'parameters': {\n","            'type': 'object',\n","            'properties': {\n","                'name': {\n","                    'type': 'string',\n","                    'description': 'Name of the person'\n","                },\n","                'job_title': {\n","                    'type': 'string',\n","                    'description': 'Job title of the person'\n","                },\n","                'num_children': {\n","                    'type': 'integer',\n","                    'description': 'Number of children the person is a parent to'\n","                },\n","                'vehicle_make': {\n","                    'type': 'string',\n","                    'description': 'Make of the person\\'s vehicle'\n","                },\n","                'vehicle_model': {\n","                    'type': 'string',\n","                    'description': 'Model of the person\\'s vehicle'\n","                },\n","                'company_name': {\n","                    'type': 'string',\n","                    'description': 'Name of the company the person works for'\n","                },\n","                'favorite_vg_series': {\n","                    'type': 'string',\n","                    'description': 'Name of the person\\'s favorite video game series'\n","                }\n","            }\n","        }\n","    }\n","]"]},{"cell_type":"markdown","metadata":{"id":"p7bKZqEnH0nY"},"source":["Now let's demonstrate what happens when we apply 3 different samples to all of the custom functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ondbhtOmH0nY"},"outputs":[],"source":["# Defining a list of samples\n","samples = [\n","    about_me,\n","    'My name is David Hundley. I am a principal machine learning engineer, and I have two daughters.',\n","    'She drives a Kia Sportage.'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNF0ss92H0nY","outputId":"20ba27db-b108-4cf6-f3f1-a65fa59402ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728857771812,"user_tz":-420,"elapsed":1832,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample #1's results:\n","ChatCompletion(id='chatcmpl-94b63aa5-7350-4225-a3d2-7ff1fa501743', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857770, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_6a6771ae9c', usage=CompletionUsage(completion_tokens=77, prompt_tokens=1429, total_tokens=1506, completion_time=0.064166667, prompt_time=0.203900812, queue_time=0.0021255980000000063, total_time=0.268067479), x_groq={'id': 'req_01ja3z826ge6vsjkx56hmqx5a7'})\n","Sample #2's results:\n","ChatCompletion(id='chatcmpl-54e72624-a4db-46f3-a0d0-ae356ab5aaee', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1728857770, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_af05557ca2', usage=CompletionUsage(completion_tokens=77, prompt_tokens=1391, total_tokens=1468, completion_time=0.064166667, prompt_time=0.283461586, queue_time=0.003471945000000032, total_time=0.347628253), x_groq={'id': 'req_01ja3z82rsedzsybq8g5wft0e9'})\n","Sample #3's results:\n","ChatCompletion(id='chatcmpl-9eb834f6-d2b0-4a0b-8def-cf6cbdc0d3ca', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"vehicle_make\":\"Kia\"}', name='extract_vehicle_info'), tool_calls=None))], created=1728857771, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_179b0f92c9', usage=CompletionUsage(completion_tokens=76, prompt_tokens=1377, total_tokens=1453, completion_time=0.063333333, prompt_time=0.189629938, queue_time=0.002821443000000007, total_time=0.252963271), x_groq={'id': 'req_01ja3z83anf5wvhw1vg5kb47fj'})\n"]}],"source":["# Iterating over the three samples\n","for i, sample in enumerate(samples):\n","\n","    print(f'Sample #{i + 1}\\'s results:')\n","\n","    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n","    groq_response1 = client.chat.completions.create(\n","        model = 'llama3-8b-8192',\n","        messages = [{'role': 'user', 'content': sample}],\n","        functions = my_custom_functions,\n","        function_call = 'auto'\n","    )\n","\n","    # Printing the sample's response\n","    print(groq_response1)"]},{"cell_type":"markdown","metadata":{"id":"QD7qtImGH0nY"},"source":["With each of the respective prompts, ChatGPT selected the correct custom function, and we can specifically note that in the `name` value under `function_call` in the API's response object. In addition to this being a handy way to identify which function to use the arguments for, we can programmatically map our actual custom Python function to this value to run the correct code appropriately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGfDFvrTH0nZ","outputId":"1a0a319f-4b34-496b-c2c2-43c3d0032640","colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"error","timestamp":1728857908204,"user_tz":-420,"elapsed":1950,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample #1's results:\n","This person's name is David Hundley. Their job title is principal machine learning engineer, and they have 2 children.\n","Sample #2's results:\n","This person's name is Principal Machine Learning Engineer. Their job title is David Hundley, and they have 2 children.\n","Sample #3's results:\n"]},{"output_type":"error","ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_person_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"num_children\": \"0\",\\n\\t\\t\\t\\t\"name\": \"Unknown\",\\n\\t\\t\\t\\t\"job_title\": \"Unknown\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-02e0bb8931f5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Getting the response back from ChatGPT (gpt-3.5-turbo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     openai_response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'llama3-8b-8192'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_calls\": [\\n\\t\\t{\\n\\t\\t\\t\"id\": \"pending\",\\n\\t\\t\\t\"type\": \"function\",\\n\\t\\t\\t\"function\": {\\n\\t\\t\\t\\t\"name\": \"extract_person_info\"\\n\\t\\t\\t},\\n\\t\\t\\t\"parameters\": {\\n\\t\\t\\t\\t\"num_children\": \"0\",\\n\\t\\t\\t\\t\"name\": \"Unknown\",\\n\\t\\t\\t\\t\"job_title\": \"Unknown\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t]\\n}\\n</tool-use>'}}"]}],"source":["# Iterating over the three samples\n","for i, sample in enumerate(samples):\n","\n","    print(f'Sample #{i + 1}\\'s results:')\n","\n","    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n","    openai_response = client.chat.completions.create(\n","        model = 'llama3-8b-8192',\n","        messages = [{'role': 'user', 'content': sample}],\n","        functions = my_custom_functions,\n","        function_call = 'auto'\n","    ).choices[0].message\n","\n","    # Checking to see that a function call was invoked\n","    if openai_response.function_call:\n","        # Checking to see which specific function call was invoked\n","        function_called = openai_response.function_call.name\n","\n","        # Extracting the arguments of the function call\n","        function_args = json.loads(openai_response.function_call.arguments)\n","\n","        # Invoking the proper functions\n","        if function_called == 'extract_person_info':\n","            extract_person_info(*list(function_args.values()))\n","        elif function_called == 'extract_vehicle_info':\n","            extract_vehicle_info(*list(function_args.values()))\n","        elif function_called == 'extract_all_info':\n","            extract_all_info(*list(function_args.values()))"]},{"cell_type":"markdown","metadata":{"id":"NpXnf2mAH0nZ"},"source":["## OpenAI Function Calling with LangChain\n","Given its popularity amongst the Generative AI community, I thought I'd re-visit this notebook and add some code to show how you might make use of this exact same functionality in LangChain"]},{"cell_type":"code","source":["!pip install --quiet -U langchain langchain-groq langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhOmubQskrv6","executionInfo":{"status":"ok","timestamp":1728857999926,"user_tz":-420,"elapsed":9005,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"7c4866eb-8a48-40c7-e15a-bd5e15dc7b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zivmk4LtH0nZ"},"outputs":[],"source":["# Importing the LangChain objects\n","from langchain_groq import ChatGroq\n","from langchain.chains import LLMChain\n","from langchain.prompts.chat import ChatPromptTemplate\n","from langchain.chains.openai_functions import create_structured_output_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cp067ipkH0nZ"},"outputs":[],"source":["# Setting the proper instance of the OpenAI model\n","llm = ChatGroq(model = 'llama3-8b-8192')\n","\n","# Setting a LangChain ChatPromptTemplate\n","chat_prompt_template = ChatPromptTemplate.from_template('{my_prompt}')\n","\n","# Setting the JSON schema for extracting vehicle information\n","langchain_json_schema = {\n","    'name': 'extract_vehicle_info',\n","    'description': 'Extract the make and model of the person\\'s car',\n","    'type': 'object',\n","    'properties': {\n","        'vehicle_make': {\n","            'title': 'Vehicle Make',\n","            'type': 'string',\n","            'description': 'Make of the person\\'s vehicle'\n","        },\n","        'vehicle_model': {\n","            'title': 'Vehicle Model',\n","            'type': 'string',\n","            'description': 'Model of the person\\'s vehicle'\n","        }\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLkSz-PUH0nZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728858144132,"user_tz":-420,"elapsed":339,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}},"outputId":"431dc401-b0c6-4de8-8c2a-31518ab35568"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-0d6661282edd>:2: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 1.0. Use :meth:`~ChatOpenAI.with_structured_output` instead.\n","  chain = create_structured_output_chain(output_schema = langchain_json_schema,\n"]}],"source":["# Defining the LangChain chain object for function calling\n","chain = create_structured_output_chain(output_schema = langchain_json_schema,\n","                                       llm = llm,\n","                                       prompt = chat_prompt_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lux3-94PH0nZ","outputId":"daab3fe7-044a-410b-e9cc-801f9440d8c7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728858158751,"user_tz":-420,"elapsed":667,"user":{"displayName":"Hảo Nguyễn","userId":"10234462271197250553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'my_prompt': 'I drive a Tesla Model 3', 'function': {'extract_vehicle_info': {'vehicle_make': 'Tesla', 'vehicle_model': 'Model 3'}}}\n"]}],"source":["# Getting results with a demo prompt\n","print(chain.invoke(input = {'my_prompt': 'I drive a Tesla Model 3'}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TfxGNd9H0nZ"},"outputs":[],"source":["clear_output()"]},{"cell_type":"code","source":[],"metadata":{"id":"7svqhafB0Zzx"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1sJQ9--ciLzUDacRFbHAYtyprlHf0Dauv","timestamp":1728856804408}]}},"nbformat":4,"nbformat_minor":0}